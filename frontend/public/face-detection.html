<h1>Face Detection</h1>
<video id="video" autoplay style="display: none;"></video>
<canvas id="canvas" width="600px" height="400px"></canvas>
<h2 id="message">Status: All good!</h2>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>

<script type="text/javascript">

let video = document.getElementById("video");
let model;
let canvas = document.getElementById("canvas");
let ctx = canvas.getContext("2d");
let message = document.getElementById("message")


const setupCamera = () => {
  navigator.mediaDevices.getUserMedia({
    video: {width: 600, height: 400},
    audio: false
  }).then(stream => {
    video.srcObject = stream;
  });
}

const detectFaces = async () => {
  const prediction = await model.estimateFaces(video, false);

  console.log(prediction);

  ctx.drawImage(video, 0, 0, 600, 400)

  prediction.forEach(pred => {
    // Warning: rainbow color only fun for a while
    const rainbow = Math.floor(Math.random()*16777215).toString(16);
    
    ctx.beginPath();
    ctx.lineWidth = "4";
    ctx.strokeStyle = "#FFC0CB";
    ctx.rect(
      pred.topLeft[0],
      pred.topLeft[1],
      pred.bottomRight[0] - pred.topLeft[0],
      pred.bottomRight[1] - pred.topLeft[1]
    );
    ctx.stroke();
    message.innerHTML = "Status: All good!";
  })

  // If face is not detected
  if (prediction.length === 0){
      console.log("No face detected");
      message.innerHTML = "No faces detected";
      ctx.fillStyle = "rgba(255, 65, 0, 0.6)";
      ctx.fillRect(0, 0, 600, 400)
  } 

  // If there's more than 1 face detected
  if (prediction.length >= 2){
    message.innerHTML = `There are ${prediction.length} faces in the video`;
  }
}

setupCamera();
// Make sure model is loaded before calling function
video.addEventListener("loadeddata", async () => {
  model = await blazeface.load();

  // Around 10 frames per second, change number for other framerates 
  setInterval(detectFaces, 100);
})

  // async function main() {
  //   // Load the model.
  //   const model = await blazeface.load();

  //   // Pass in an image or video to the model. The model returns an array of
  //   // bounding boxes, probabilities, and landmarks, one for each detected face.

  //   const returnTensors = false; // Pass in `true` to get tensors back, rather than values.
  //   const predictions = await model.estimateFaces(video, returnTensors);

  //   if (predictions.length > 0) {
      
  //     // `predictions` is an array of objects describing each detected face, for example:

  //     [
  //       {
  //         topLeft: [232.28, 145.26],
  //         bottomRight: [449.75, 308.36],
  //         probability: [0.998],
  //         landmarks: [
  //           [295.13, 177.64], // right eye
  //           [382.32, 175.56], // left eye
  //           [341.18, 205.03], // nose
  //           [345.12, 250.61], // mouth
  //           [252.76, 211.37], // right ear
  //           [431.20, 204.93] // left ear
  //         ]
  //       }
  //     ]
      

  //     for (let i = 0; i < predictions.length; i++) {
  //       const start = predictions[i].topLeft;
  //       const end = predictions[i].bottomRight;
  //       const size = [end[0] - start[0], end[1] - start[1]];

  //       // Render a rectangle over each detected face.
  //       ctx.fillRect(start[0], start[1], size[0], size[1]);
  //     }
  //   }
  // }

  // main();



</script>